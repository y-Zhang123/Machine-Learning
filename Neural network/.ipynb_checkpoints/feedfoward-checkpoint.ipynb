{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b229a3",
   "metadata": {},
   "source": [
    "# Feedforward Network\n",
    "---\n",
    "one-direction\\\n",
    "1. Input Layer: Each neuron represents a feature of the input data.\n",
    "2. Hidden Layer(s): are responsible for learning the complex patterns in the data. Each neuron applies a weighted sum of inputs followed by a non-linear activation function to introduce non-linearity into the network enabling it to learn and model complex data patterns. \n",
    "3. Output Layer: Each neuron represents a class in classification or a prediction in regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76b295",
   "metadata": {},
   "source": [
    "At a high level, neural network layers are categorized as input, hidden, and output layers. The input layer receives the data, the output layer produces task-specific predictions, and the hidden layer(s) learn representations — this is a broad category that includes many specialized layer types such as dense (fully connected), convolutional, recurrent, pooling, normalization, dropout, and attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35eded1",
   "metadata": {},
   "source": [
    "# Training a FNN\n",
    "Basically adjusting the weights of the neurons to minimize the error between the predicted output and the actual output. This process is typically performed using backpropagation and gradient descent.\\\n",
    "1. **Forward Propagatin**: the input data passes through the network and the output is calculated.\n",
    "2. **Loss Calculation**: MSE for regression, cross-entropy for classification.\n",
    "3. **Backpropagation**: the error is propagated back through the network to update the weights. The gradient of the loss function with respect to each weights is calculated and the weights are adjusted using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94056d6",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "The forward pass computes predictions from inputs using the model's current weights. For a dense layer:\n",
    "\n",
    "$$z = W x + b, \\quad a = \\phi(z)$$\n",
    "\n",
    "Each layer's output becomes the next layer's input; the final layer produces $y_{pred}$. The forward pass alone does not update weights — weight updates occur during backpropagation after computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal NumPy demo: single dense layer forward + manual backward (MSE)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# batch_size=3, input_dim=4, output_dim=2\n",
    "x = np.random.randn(3, 4)\n",
    "W = np.random.randn(2, 4)\n",
    "b = np.random.randn(2)\n",
    "# targets (one-hot-like for simplicity)\n",
    "y_true = np.array([[1, 0], [0, 1], [1, 0]])\n",
    "# Forward: z = x W^T + b (identity activation)\n",
    "z = x.dot(W.T) + b\n",
    "y_pred = z\n",
    "# Mean squared error (average over batch): 1/N * 0.5 * sum||y_pred - y_true||^2\n",
    "loss = np.mean(0.5 * np.sum((y_pred - y_true)**2, axis=1))\n",
    "print('Loss before update:', loss)\n",
    "# Backward: gradients wrt outputs, weights, biases (batch-averaged)\n",
    "dL_dy = (y_pred - y_true) / x.shape[0]  # shape (3,2)\n",
    "dW = dL_dy.T.dot(x)  # shape (2,4)\n",
    "db = np.sum(dL_dy, axis=0)  # shape (2,)\n",
    "# Gradient step (SGD)\n",
    "lr = 0.1\n",
    "W -= lr * dW\n",
    "b -= lr * db\n",
    "# Forward again to see loss decrease\n",
    "z2 = x.dot(W.T) + b\n",
    "y_pred2 = z2\n",
    "loss2 = np.mean(0.5 * np.sum((y_pred2 - y_true)**2, axis=1))\n",
    "print('Loss after one SGD step:', loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea28bea",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Backpropagation computes gradients of the loss with respect to each parameter using the chain rule and the values cached during the forward pass, then the optimizer uses those gradients to update weights.\n",
    "\n",
    ". Typical steps:\n",
    "\n",
    "1. Compute the loss $L$ from predictions and targets.\n",
    "2. Compute output-layer error:\\\n",
    "  $\\delta^L = \\partial L / \\partial z^L = (\\partial L / \\partial a^L) \\odot \\phi'(z^L)$.\n",
    "3. Backpropagate errors layer-by-layer:\\\n",
    "  $\\delta^{l} = (W^{l+1})^T \\delta^{l+1} \\odot \\phi'(z^l)$.\n",
    "4. Compute gradients:\\\n",
    "$\\partial L / \\partial W^l = \\delta^l (a^{l-1})^T$,\\ $\\partial L / \\partial b^l = \\sum_{batch} \\delta^l$.\n",
    "\n",
    "An optimizer (SGD, Adam, etc.) uses these gradients to update parameters (e.g. $W eftarrow W - ta \\nabla_W L$). Modern frameworks perform these steps automatically via reverse-mode automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1074a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import I\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119c6d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "minist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = minist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d13e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhangyiwen\\AppData\\Local\\anaconda3\\envs\\zyw\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    # Flatten layer is input layer, it will convert 28*28(width * height) 2D array to 784 1D array, the unit is pixel\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "    # these two dense are hidden layers? \n",
    "    # the first dense layer has 128 neurons, the second dense layer has 10 neurons, they are fully connected layers by weights and biases\n",
    "    # they are hiden layers because we don't see them in the input or output\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c510c",
   "metadata": {},
   "source": [
    "## Model is compliled with\n",
    "- Adam optimizer\n",
    "- Sparse Categorical Crossentropy loss function\n",
    "- Sparse Categorical Accuracy metric\n",
    "- Then trained for 5 epochs on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136c57dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.9268\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1151 - sparse_categorical_accuracy: 0.9661\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0785 - sparse_categorical_accuracy: 0.9765\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0578 - sparse_categorical_accuracy: 0.9824\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0434 - sparse_categorical_accuracy: 0.9867\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0905 - sparse_categorical_accuracy: 0.9720\n",
      "\n",
      "Test accuracy: 0.972000002861023\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(),\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=[SparseCategoricalAccuracy()])\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d316bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (zyw)",
   "language": "python",
   "name": "zyw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
